{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith - Trazas, Pruebas y Evaluaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Crea trazas con LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "uid = uuid.uuid4()\n",
    "PROJECT_NAME = \"Lil Demo-\" + str(uid)\n",
    "\n",
    "session = client.create_project(\n",
    "   project_name=PROJECT_NAME,\n",
    "   description=\"Demo for starting to use the Langsmith API\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0fef67ca-8cdf-40c9-8f67-8dffa7e7bd86-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hey!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are a meeting summarization agent. Your task is to analyze a meeting transcript and create a structured summary in JSON format with the following fields. Ensure that your response is always formatted as follows:\n",
    "\n",
    "  \"date\": \"Date of the meeting\",\n",
    "  \"participants\": [\"Participant 1\", \"Participant 2\", \"Participant 3\", ...],\n",
    "  \"summary\": \"A concise summary of the key discussion points and outcomes of the meeting.\",\n",
    "  \"action_points\": [\"Action Point 1\", \"Action Point 2\", \"Action Point 3\", ...],\n",
    "  \"transcript_body\": \"The full content of the meeting transcript.\"\n",
    "\n",
    "Make sure to extract names, key points, and relevant actions clearly and consistently.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = '''Date: November 4, 2024\n",
    "        Participants: Alice, Bob, Charlie\n",
    "\n",
    "        Transcript:\n",
    "\n",
    "        Alice: Good morning, everyone. Let’s start with the project update. Bob, could you share the current status?\n",
    "\n",
    "        Bob: Sure, Alice. The development team has completed the initial phase, and we’re now moving into testing. We anticipate some minor bugs, but overall, things are on track to meet the deadline.\n",
    "\n",
    "        Charlie: That’s good news. How about the client presentation preparation?\n",
    "\n",
    "        Alice: Yes, we should start preparing for that. Charlie, could you put together a draft of the slides by Thursday?\n",
    "\n",
    "        Charlie: Absolutely. I’ll have it ready for review.\n",
    "\n",
    "        Bob: Also, I wanted to mention that we need to allocate more time for user feedback analysis. Should I schedule a separate session for that next week?\n",
    "\n",
    "        Alice: Yes, please do. It’s important to get thorough feedback to refine the final product.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith - Evaluate RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone dataset\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "    \"https://smith.langchain.com/public/d7e3a510-a07a-42a9-872a-e2cf189f2929/d\"\n",
    ")\n",
    "\n",
    "dataset_name = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "\n",
    "# Cargar documentos\n",
    "loader = TextLoader(\"../data/office_procedures.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Dividir en chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed y almacenar en vectorstore Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Índice\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "class RagBot:\n",
    "\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o\"):\n",
    "        self._retriever = retriever\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question, docs):\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI agent with expertise this Office´s Procedures.\"\n",
    "                    \" Use ONLY the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{docs}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "# Inicializar el bot\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para gestionar reuniones de equipo de manera efectiva, se recomienda:\n",
      "\n",
      "1. Enviar una agenda clara con antelación.\n",
      "2. Establecer objetivos específicos para la reunión.\n",
      "3. Asignar un moderador para garantizar que la discusión fluya de manera ordenada.\n",
      "4. Evitar interrupciones durante la reunión.\n",
      "5. Limitar el uso de dispositivos electrónicos a lo necesario y evitar distracciones.\n"
     ]
    }
   ],
   "source": [
    "response = rag_bot.get_answer(\"De acuerdo con el contexto, ¿cuáles son las prácticas para gestionar reuniones de equipo?\")\n",
    "print (response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Función para evaluar la respuesta generada por el modelo\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Función para evaluar los documentos recuperados y las alucinaciones\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response vs Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Prompt para evaluar la respuesta vs la referencia\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluator de accuracy de la respuesta generada por el modelo vs la respuesta de referencia\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtén pregunta, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM para comparar respuesta vs referencia\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Prompt estructurado\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Ejecutar el evaluador\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-v-reference-38cb025e' at:\n",
      "https://smith.langchain.com/o/896f324f-7bc4-5a0a-8ffc-a3ab7bd5a36e/datasets/6914005e-d3d8-466c-9c22-5b6585ee1741/compare?selectedSessions=0f714644-c8ff-4b21-adcd-392c012afe54\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7fbfcf3f444b81aa0afe693c19b531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-answer-v-reference\",\n",
    "    metadata={\"version\": \"TEST-questions, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluator de la utilidad de la respuesta generada por el modelo con respecto a la pregunta\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtén pregunta, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM para evaluar la utilidad de la respuesta\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Prompt estructurado\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Ejecutar evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-helpfulness-8fc67e2c' at:\n",
      "https://smith.langchain.com/o/896f324f-7bc4-5a0a-8ffc-a3ab7bd5a36e/datasets/6914005e-d3d8-466c-9c22-5b6585ee1741/compare?selectedSessions=35d2ae10-8f27-4bb8-bf22-08dbbcf411b4\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:27,  5.55s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_helpfulness_evaluator],\n",
    "    experiment_prefix=\"rag-answer-helpfulness\",\n",
    "    metadata={\"version\": \"TEST-questions, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_hallucination_grader(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG input\n",
    "    rag_pipeline_run = next(\n",
    "        run for run in root_run.child_runs if run.name == \"get_answer\"\n",
    "    )\n",
    "    retrieve_run = next(\n",
    "        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"\n",
    "    )\n",
    "    contexts = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
    "\n",
    "    # RAG output\n",
    "    prediction = rag_pipeline_run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[document_relevance_grader, answer_hallucination_grader],\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
